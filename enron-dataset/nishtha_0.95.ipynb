{"cells":[{"metadata":{},"cell_type":"markdown","source":"## **Person of Interest identifier in Enron Email Dataset**\n<p>In 2000, Enron was one of the largest companies in the United States. Two years later, it became bankrupt due to widespread corporate fraud. In the resulting Federal investigation, normally confidential information, including tens of thousands of emails and detailed financial data for top executives, entered the public record.\n\nThe objective of this project is to build an algorithm to identify Enron employees who may have committed fraud based on the public Enron financial and email dataset. Such employees are referred to as \"person's of interest\", or, POIs.</p>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **Importing Libraries**\nFirst we will start by importing libraries required.","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sys\nimport pickle\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV, train_test_split, validation_curve, cross_val_score, cross_val_predict\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nsys.path.append(\"/kaggle/input/enron-dataset/\")\nfrom feature_format import featureFormat\nfrom feature_format import targetFeatureSplit\n\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Importing the dataset**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Next we will load the datset. The data here is given in the form of a dictionary containing combined Enron email and financial data. The dictionary key is the person's name, and the value is another dictionary, which contains the names of all the features and their values for that person. The features in the data fall into two major types; financial features and email features. Each person is also labelled as a poi (boolean).\n\nWe will load the pickle file and work it as a pandas dataframe.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"with open(\"/kaggle/input/enron-dataset/final_project_dataset_unix.pkl\", \"rb\") as file:\n    dataset = pickle.load(file)\n\ndf = pd.DataFrame.from_dict(dataset,orient='index')\ndf = df.replace('NaN',np.nan)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Outlier Detection**","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"For finding outliers, we will first plot employee salaries and bonuses to see if any employees are receiving payments that are magnitudes larger than others. Also to see if any employees are receiving bunuses much more than others.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':16,\n            'xtick.labelsize':10,\n            'font.size':10,\n            'ytick.labelsize':10,\n            'figure.figsize': (10, 7)}\n       )\ndf.plot(kind = 'scatter', x = 'salary', y = 'bonus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see outliers right away. On analysing further, we see that the outlier is a result of a \"TOTAL\" field being present in the dictionary that sums each field for all employees in the dataset.\n\nThe \"total\" field is removed, and the same fields are re-plotted below.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.pop('TOTAL')\n\ndf = pd.DataFrame.from_dict(dataset,orient='index')\ndf = df.replace('NaN',np.nan)\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':16,\n            'xtick.labelsize':10,\n            'font.size':10,\n            'ytick.labelsize':10,\n            'figure.figsize': (10, 7)}\n       )\ndf.plot(kind = 'scatter', x = 'salary', y = 'bonus')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Next, we try to plot total payments and we still find some outliers.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':16,\n            'xtick.labelsize':10,\n            'font.size':10,\n            'ytick.labelsize':10,\n            'figure.figsize': (10, 7)}\n       )\ndf.plot(kind = 'scatter', x = 'salary', y = 'total_payments')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"To investigate further we check in the dataframe the employees and which category they belong to w.r.t. POI.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"bonus = df[(df.salary > 1000000) | (df.bonus > 5000000)][['bonus','salary','poi','total_payments']]\nbonus.sort_values(by = 'total_payments', ascending= False, inplace=True)\nbonus","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We find that most of these outliers are people who are in the POI category. And since we have only 18 POI so we will keep these data and not remove it. <br>\nAlso, Kenneth Lay and Jeffrey Skilling have come up in this list which already brings out their involvement in the fraud as they have been receiving way more bonus than their salary.","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **Exploratory Data Analysis**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.shape, df.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The features in the data fall into three major types, namely financial features, email features and POI labels. THey are given below:\n\n1. **Financial features: (all units are in US dollars)**\n   * salary\n   * deferral_payments\n   * total_payments\n   * loan_advances\n   * bonus\n   * restricted_stock_deferred\n   * deferred_income\n   * total_stock_value\n   * expenses\n   * exercised_stock_options\n   * other\n   * long_term_incentive\n   * restricted_stock\n   * director_fees\n   \n2. **Email features: (units are generally number of emails messages; exception is ‘email_address’, which is a string)**\n   * to_messages\n   * email_address\n   * from_poi_to_this_person\n   * from_messages\n   * from_this_person_to_poi\n   * shared_receipt_with_poi\n   \n3. **POI label: (boolean value representing wheter a person is POI)**\n   * poi","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df.dtypes","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"All the columns are having correct data type as it should have been. We changed the True/False labels to int in poi so its datatype is int64 and name and email having object dtye. All other columns are float.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":" df['poi'] = df['poi'].apply(lambda x: 1 if x else 0)\n\npoi = df.poi.value_counts()\nprint(poi)\n\nsns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':16,\n            'xtick.labelsize':10,\n            'font.size':10,\n            'ytick.labelsize':10,\n            'figure.figsize': (6, 6)}\n       )\npoi.plot(kind = 'pie')\nplt.legend()\nplt.xlabel('Person of Interest')\nplt.ylabel('Count')\n\nplt.title ('The dataset contains a total of %s POIs.' % sum(df['poi']))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"all = df.count()\npois = df[df['poi'] == 1].count()\n\nresult = pd.concat([all, pois], axis=1)\nresult.columns = ['All Records','POIs']\n\nsns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':16,\n            'xtick.labelsize':10,\n            'font.size':10,\n            'ytick.labelsize':10,\n            'figure.figsize': (18, 12)}\n       )\n\nfig = result.plot(kind = 'bar',\n       figsize = (20, 14),\n       color = ['#5cb85c', '#5bc0de'])\nfig.set_title('Distribution of Total Records present and number of POI present', fontsize=20)\nfig.set_xlabel('Feature', fontsize = '16')\nfig.set_ylabel('Count', fontsize = 16)\nfig.legend(fontsize = 14)\n\nfor p in fig.patches:\n    fig.annotate(np.round(p.get_height(),decimals=2), \n                (p.get_x()+p.get_width()/2., p.get_height()), \n                ha='center', \n                va='center', \n                xytext=(0, 10), \n                textcoords='offset points',\n                fontsize = 14\n               )","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In this plot, we see number of total values present in each column represented by 'All Records' label and the number of 'POI' in each column involved with the 'POIs' label. We seem to have missing values for all columns except for name and poi column and thus no single feature has information for all the employees. This was expected as such confidential information is difficult to collect for all users.\n<br>","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### **Pairwise Plots**\n\nThey are an easy way to see the dependence of features on each other. We will first plot the pairwise plot for the financial features. We have not used all the features and dropped ones with too few entries present.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':15,\n            'xtick.labelsize':10,\n            'font.size':10,\n            'ytick.labelsize':10}\n       )\nfinancial = ['poi', 'salary', 'deferral_payments', 'total_payments', 'bonus', 'total_stock_value', 'expenses', 'exercised_stock_options','long_term_incentive', 'restricted_stock'] \nsns.pairplot(df[financial], hue = 'poi', palette = 'husl')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.set(rc={\"font.style\":\"normal\",\n            'axes.labelsize':8,\n            'xtick.labelsize':8,\n            'font.size':8,\n            'ytick.labelsize':8}\n       )\nemail = ['to_messages','from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi', 'poi']\nsns.pairplot(df[email], hue = 'poi', palette = 'husl')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Feature Selection and Engineering**\n\nWe drop the email_address feature as its a string and its not of much use. Then we use the functions in the feature_format module to split the dataset into target and feature variables. And then split it into trani and test set with test size of 15% of the total dataset. Also we will add features bonus_to_salary containing the ratio of bonus and salary and also bonus_to_total with ratio of bonus to total payment.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"features_list = [\"poi\", \"salary\", \"bonus\", 'deferral_payments', 'total_payments', 'loan_advances', \n                 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses',\n                 'exercised_stock_options', 'long_term_incentive', 'other', 'shared_receipt_with_poi', \n                 'restricted_stock', 'director_fees', 'to_messages','from_poi_to_this_person', \n                 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n\ndata = featureFormat(dataset, features_list)\ny, X = targetFeatureSplit(data)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nprint(len(y_train), len(y_test))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Logistic Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"X_scaled = preprocessing.scale(X_train)\nXtest_scaled = preprocessing.scale(X_test)\nlr = LogisticRegression(max_iter = 5000)\ngrid = {'C':[0.01, 0.03, 0.1, 0.3, 1, 3, 10]}\ngrid_lr = GridSearchCV(lr,param_grid=grid,scoring='accuracy',cv=5)\ngrid_lr.fit(X_scaled,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_lr.best_params_)\npred = grid_lr.predict(Xtest_scaled)\nprint('Test Accuracy = ',grid_lr.score(Xtest_scaled,y_test))\nprint(metrics.classification_report(y_test,pred, zero_division=0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Random Forest Regression**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"rf = RandomForestClassifier(n_estimators=200)\ngrid = {'n_estimators':[1, 10, 50],'max_depth':[25,30,35,40,45,50]}\ngrid_rf = GridSearchCV(rf,param_grid=grid,scoring='accuracy',cv=5)\ngrid_rf.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_rf.best_params_)\npred = grid_rf.predict(X_test)\nprint('Accuracy = ',grid_rf.score(X_test,y_test))\nprint(metrics.classification_report(y_test,pred, zero_division = 0))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **K Means Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"km = KNeighborsClassifier()\ngrid = {'n_neighbors':[4,5,6,7,8,9,10,11]}\ngrid_km = GridSearchCV(km,param_grid=grid,scoring='accuracy',cv=5)\ngrid_km.fit(X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_km.best_params_)\npred = grid_km.predict(X_test)\nprint('Accuracy = ',grid_km.score(X_test,y_test))\nprint(metrics.classification_report(y_test,pred, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **AdaBoost Classifier**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nada = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=200)\nparam_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\"base_estimator__splitter\":[\"best\", \"random\"], \"n_estimators\": [1, 2]}\ngrid_ada = GridSearchCV(ada, param_grid=param_grid, scoring = 'accuracy', cv=5)\ngrid_ada.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(grid_ada.best_estimator_)\npred = grid_ada.predict(X_test)\nprint('Accuracy = ',grid_ada.score(X_test,y_test))\nprint(metrics.classification_report(y_test,pred, zero_division = 0))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Classification Report**\n\n<table>\n    <tr><b>\n        <th>Classifier</th>\n        <th>Precision(weighted_avg)</th>\n        <th>Recall(weighted_avg)</th>\n        <th>Test Accuracy</th>\n    </b></tr>\n    <tr>\n        <th>Logistic Regression</th>\n        <th>0.83</th>\n        <th>0.91</th>\n        <th>0.91</th>\n    </tr>\n        <tr>\n        <th>Random Forest Classifier</th>\n        <th>0.83</th>\n        <th>0.91</th>\n        <th>0.91</th>\n    </tr>\n        <tr>\n        <th>Ada Boost Classifier</th>\n        <th>0.96</th>\n        <th>0.95</th>\n        <th>0.95</th>\n    </tr>\n        <tr>\n        <th>K Means Classifier</th>\n        <th>0.96</th>\n        <th>0.95</th>\n        <th>0.95</th>\n    </tr>\n</table>    ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"pickle.dump(ada, open(\"my_classifier.pkl\", \"wb\") )\npickle.dump(dataset, open(\"my_dataset.pkl\", \"wb\") )\npickle.dump(features_list, open(\"my_feature_list.pkl\", \"wb\") )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}